{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the examples below are taken from the [NLTK book](http://www.nltk.org/book/) Before we start, we should install all the required material. Run the cell below to install the tools and corpora. This can take a minute..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the cell below to install the additional material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Python's Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I demonstrate the power of the NLTK by inspecting some of the **prepared corpora** of this library. Later on, I show how you can build your own corpus, and unleash all the nice tools on **your own data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Digital Humanities, we often treat texts as *raw data*, as input for our programs. Interpretations arise from abstraction, for example, by counting word frequencies, analysing specific segments of a corpus (i.e. Key Word In Context, or KWIC analysis) or searching for patterns (i.e. collocations). \n",
    "\n",
    "NLTK provides several tools for both **processing** data and **interpreting** texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the tools NLTK provides us with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK helps us with \"tokenization\" a tool that helps to break a string into separate words (also called tokens).\n",
    "\n",
    "We first need to import the `word_tokenize` tool using the `import` syntax below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this function to any English text and it will identify the tokens and separate strings from punctation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"Downing Street has said it is “extremely concerning” that MPs could attempt to override the government to suspend or delay the article 50 process to leave the EU in their effort to prevent a no-deal Brexit.\"\n",
    "print(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string is just a sequence of characters. However, for simple tasks, such as word counting, we need to split up this string by token.\n",
    "\n",
    "Below we first lowercase the string and then tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentence_lower = example_sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(example_sentence_lower)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Select the first sentence of [Alice in Wonderland](http://www.gutenberg.org/cache/epub/28885/pg28885.txt) and tokenize the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization we can easily compute the word frequency with the `nltk.FreqDist()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to know the frequency of one specific word we put this word in between square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq = nltk.FreqDist(words)\n",
    "fq['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "What is the frequency of 'to' and 'street'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the relative frequency, the count has to be divided by the total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(words)\n",
    "fq['the']/total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "The code below downloads the text of Shakespeare's Romeo and Juliet and saves it in the variable `randj`.\n",
    "Perform the following steps:\n",
    "- Lowercase the text;\n",
    "- Tokenize the lowercased text; save the tokens in a new variable with the name `randj_tokens`.\n",
    "- How many tokens does Romeo and Julia contain? How many characters?\n",
    "- Make a frequency table, compute the relative frequency of the word \"love\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "randj = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenizing the text we can apply a bunch of NLTK tools. Below we use the example of Kipling's [Jungle Book](http://www.gutenberg.org/cache/epub/35997/pg35997.txt). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first tokenize the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "jungle_book = requests.get('http://www.gutenberg.org/cache/epub/35997/pg35997.txt').text\n",
    "jungle_book_tokens = word_tokenize(jungle_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(jungle_book_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word_tokenize` returns a `list` object. Lists are useful for storing information, but not for analysing texts. To allow for more refined text analysis we have to convert the list to a NLTK Text object. This type conversion (from list to NLTK text is done below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jungle_nltk_text = nltk.text.Text(jungle_book_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(jungle_nltk_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we tranformed the text into an NLTK Text object we can rely on several handy tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.concordance()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An oft-used technique for distant reading is **Keyword In Context Analysis** in which we centre a whole corpus on a specific word of interest. NLTK comes with a `concordance()` method that allows you to do just this. Let's analyse how the word \"love\" in Jungle book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jungle_nltk_text.concordance(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the number of hits to print with the `lines` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jungle_nltk_text.concordance(\"black\",lines=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "How is the word love used in Romeo and Juliet? The code below downloads Romeo and Juliet, continue the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "randj = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.similar()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`concordance()` shows words in their context. For example, in Moby Dick the word monstrous occurres in contexts such as the \\_\\_\\_ pictures and a \\_\\_\\_ size. What other words appear in a **similar range of contexts**? We can find out by using the `.similar()` method, entering the word you want analyse within parentheses. Below we aks which words are similar to \"forest\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jungle_nltk_text.similar(\"forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Research Scenario: How left and right-wing media depict Mueller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research scenario below applies these techniques to understand how the New York times depicts Robert S. Mueller. First we load the data.\n",
    "\n",
    "Don't worry about the technicality on line two: this command basically tells Python to read everything in \"post_message\" columns as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt = pd.read_csv('https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/nytimes.tab',sep='\\t')\n",
    "nyt['post_message'] = nyt['post_message'].astype(str) # this line is a technicality, use it but do not worry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can tokenize the post messages, we have to join them together in one string. `' '.join()` is the opposite of `.split()`. Inspect the examples below to understand how these functions work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Hello how are you?'\n",
    "words = sentence.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_merged =  ' '.join(words)\n",
    "sentence_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sentence_merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the NLTK tools to Facebook \"post_messages\" we first select this column, join all the messages into one long string, which is saved in the `posts` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = ' '.join(nyt['post_message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the post messages are stored in one string. This string is tokenized with `word_tokenize` and then convert this list to a NLTK object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts_tokens = word_tokenize(posts)\n",
    "nltk_posts = nltk.text.Text(posts_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_posts.concordance(\"Mueller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "How does Fox News report about Mueller? Use [these data](https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/foxnews.tab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert these data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\*\\*\\* Exercise\n",
    "\n",
    "Instead of the post messages, compare how commenters on the Facebook page of the New York Times and Fox News write about Mueller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Scenario: Studying changes over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides inspecting the content, we can study changes over time. What are topics salient and when do they disappear. Below we have a look at the comments on the New York Times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at trends, the data has to be sorted in chronological order. We use the timestamp of the comment to sort the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/data_nytimes/page_5281959998_2018_12_28_22_00_39_comments.tab\"\n",
    "nyt_comments = pd.read_csv(url,sep='\\t')\n",
    "nyt_comments_sorted = nyt_comments.sort_values(by='comment_published')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_comments_sorted.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sorting the data, we first make sure all the data in this column is a string. \n",
    "- Then we again join all the comments into one long string. \n",
    "- Tokenize this string.\n",
    "- And convert the list of tokens to a NLTK Text object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyt_comments_sorted['comment_message'] = nyt_comments_sorted['comment_message'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = ' '.join(nyt_comments_sorted['comment_message'])\n",
    "comments_tokens = word_tokenize(comments)\n",
    "nltk_comments = nltk.text.Text(comments_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the comments are stored as a NLTK Text object we can generate a `.dispersion()` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_comments.dispersion_plot(['Merkel','Trump','Mueller'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "How to interpret this figure?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\*\\*\\* Exercise\n",
    "\n",
    "Use posts from the [New York Times](https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/nytimes.tab) and [Fox News](https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/foxnews.tab).\n",
    "For each page:\n",
    "- Sort the data by the posts' Timestamp;\n",
    "- Select the text and `.join()` them into one string\n",
    "- Tokenize the text\n",
    "- Compute word frequencies; compute the relative frequency for the word Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\*\\*\\* Exercise\n",
    "\n",
    "Use posts from the [New York Times](https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/nytimes.tab) and [Fox News](https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/foxnews.tab).\n",
    "For each page:\n",
    "- Sort the data by the post's Timestamp;\n",
    "- Select the text and `.join()` them into one string\n",
    "- Tokenize the text\n",
    "- Compute word frequencies; compute the relative frequency for the word \"Trump\"\n",
    "- Make a dispersion plot for 'Kelly','Cohen','Pelosi', and 'Trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\*\\*\\* Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will investigate the text of Facebook posts in relation to the reactions they received. More specifically we look what makes these audiences angry or happy when these media write about Trump.\n",
    "\n",
    "Use these data\n",
    "- [New York Times]('https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/nytimes.tab')\n",
    "- [Fox News]('https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/foxnews.tab')\n",
    "\n",
    "So, again for each of these pages:\n",
    "- Compute the ratio of angry and love reactions (by dividing the this reactions type to the total reactions)\n",
    "- Select posts with a ratio of love (or angry) rections higher than 0.3\n",
    "- Save the posts that match these conditions in a new DataFrame (for example `df_nyt_angry`)\n",
    "- Concatenate all these posts into one string, split this string into tokens and convert the list of tokens to a NLTK Text object\n",
    "- Use `.concordance()` to investigate what these supposedly angry or happy posts are about? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now you know everything enough! Congratulations with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Optional Research Scenario: Training an ideological classifier (Advanced Topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part below is optional. It gives code to train an ideology classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score,accuracy_score  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the \"cldata.csv\" on Canvas. This contains rows with texts and labels.\"0\" means that the post was retrieved from the New York Times, \"1\" that comments was writen on the Fox News page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('cldata.csv',index_col=0)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To understand how the data was created, consult the appendix below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the data into a train and test set. We use 80% for training, 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = int(all_data.shape[0]*0.8)\n",
    "cut_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,y_train = all_data['comment_message'][:cut_off],all_data['label'][:cut_off]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test,y_test = all_data['comment_message'][cut_off:],all_data['label'][cut_off:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently we use the CountVectorizer to create a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix). To understand the arguments, consult the [CountVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3),max_df=0.9,min_df=20,norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below initializes a classifier model and fits the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = LinearSVC(C=10,class_weight='balanced')\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we calculate labels for the data we separated for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compute how accurarte the classifier performed on these test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(predictions,y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### \\*\\*\\* Building an Emotion Classifier (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "\n",
    "Code to create classifier data from Netvizz data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def count_tokens(string):\n",
    "    tokens = wordpunct_tokenize(str(string))\n",
    "    return len(tokens)\n",
    "\n",
    "nyt = pd.read_csv('../classifierdata/nyt_comments.tab',sep='\\t')\n",
    "fn = pd.read_csv('../classifierdata/fn_comments.tab',sep='\\t')\n",
    "nyt['comment_message_length'] = nyt['comment_message'].apply(count_tokens)\n",
    "fn['comment_message_length'] = fn['comment_message'].apply(count_tokens)\n",
    "nyt_long = nyt[nyt['comment_message_length'] > 10]\n",
    "fn_long = fn[fn['comment_message_length'] > 10]\n",
    "fn_long_reduced = fn_long.iloc[:nyt_long.shape[0],:]\n",
    "all_data = pd.concat([nyt_long,fn_long_reduced])\n",
    "labels = np.asarray([0]*nyt_long.shape[0] + [1]*fn_long_reduced.shape[0])\n",
    "all_data.shape[0]==labels.shape[0]\n",
    "all_data['label'] = labels\n",
    "order = np.asarray(range(0,all_data.shape[0]))\n",
    "random.shuffle(order)\n",
    "all_data = all_data.iloc[order,:]\n",
    "cldata = all_data.loc[:,['comment_message','label']]\n",
    "cldata.to_csv('cldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
