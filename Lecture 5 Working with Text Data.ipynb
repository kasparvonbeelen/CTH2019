{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the examples below are taken from the [NLTK book](http://www.nltk.org/book/) Before we start, we should install all the required material. Run the cell below to install the tools and corpora. This can take a minute..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the cell below to install the additional material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/kasparbeelen/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Python's Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I demonstrate the power of the NLTK by inspecting some of the **prepared corpora** of this library. Later on, I show how you can build your own corpus, and unleash all the nice tools on **your own data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Digital Humanities, we often treat texts as *raw data*, as input for our programs. Interpretations arise from abstraction, for example, by counting word frequencies, analysing specific segments of a corpus (i.e. Key Word In Context, or KWIC analysis) or searching for patterns (i.e. collocations). \n",
    "\n",
    "NLTK provides several tools for both **processing** data and **interpreting** texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the tools NLTK provides us with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK helps us with \"tokenization\" a tool that helps to break a string into separate words (also called tokens).\n",
    "\n",
    "We first need to import the `word_tokenize` tool using the `import` syntax below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this function to any English text and it will identify the tokens and separate strings from punctation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downing Street has said it is “extremely concerning” that MPs could attempt to override the government to suspend or delay the article 50 process to leave the EU in their effort to prevent a no-deal Brexit.\n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"Downing Street has said it is “extremely concerning” that MPs could attempt to override the government to suspend or delay the article 50 process to leave the EU in their effort to prevent a no-deal Brexit.\"\n",
    "print(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string is just a sequence of characters. However, for simple tasks, such as word counting, we need to split up this string by token.\n",
    "\n",
    "Below we first lowercase the string and then tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentence_lower = example_sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['downing', 'street', 'has', 'said', 'it', 'is', '“', 'extremely', 'concerning', '”', 'that', 'mps', 'could', 'attempt', 'to', 'override', 'the', 'government', 'to', 'suspend', 'or', 'delay', 'the', 'article', '50', 'process', 'to', 'leave', 'the', 'eu', 'in', 'their', 'effort', 'to', 'prevent', 'a', 'no-deal', 'brexit', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(example_sentence_lower)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Select the first sentence of [Alice in Wonderland](http://www.gutenberg.org/cache/epub/28885/pg28885.txt) and tokenize the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization we can easily compute the word frequency with the `nltk.FreqDist()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 1,\n",
       "          '50': 1,\n",
       "          'a': 1,\n",
       "          'article': 1,\n",
       "          'attempt': 1,\n",
       "          'brexit': 1,\n",
       "          'concerning': 1,\n",
       "          'could': 1,\n",
       "          'delay': 1,\n",
       "          'downing': 1,\n",
       "          'effort': 1,\n",
       "          'eu': 1,\n",
       "          'extremely': 1,\n",
       "          'government': 1,\n",
       "          'has': 1,\n",
       "          'in': 1,\n",
       "          'is': 1,\n",
       "          'it': 1,\n",
       "          'leave': 1,\n",
       "          'mps': 1,\n",
       "          'no-deal': 1,\n",
       "          'or': 1,\n",
       "          'override': 1,\n",
       "          'prevent': 1,\n",
       "          'process': 1,\n",
       "          'said': 1,\n",
       "          'street': 1,\n",
       "          'suspend': 1,\n",
       "          'that': 1,\n",
       "          'the': 3,\n",
       "          'their': 1,\n",
       "          'to': 4,\n",
       "          '“': 1,\n",
       "          '”': 1})"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd like to know the frequency of one specific word we put this word in between square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fq = nltk.FreqDist(words)\n",
    "fq['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "What is the frequency of 'to' and 'street'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the relative frequency, the count has to be divided by the total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07692307692307693"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = len(words)\n",
    "fq['the']/total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "The code below downloads the text of Shakespeare's Romeo and Juliet and saves it in the variable `randj`.\n",
    "Perform the following steps:\n",
    "- Lowercase the text;\n",
    "- Tokenize the lowercased text; save the tokens in a new variable with the name `randj_tokens`.\n",
    "- How many tokens does Romeo and Julia contain? How many characters?\n",
    "- Make a frequency table, compute the relative frequency of the word \"love\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "randj = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenizing the text we can apply a bunch of NLTK tools. Below we use the example of Kipling's [Jungle Book](http://www.gutenberg.org/cache/epub/35997/pg35997.txt). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first tokenize the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "jungle_book = requests.get('http://www.gutenberg.org/cache/epub/35997/pg35997.txt').text\n",
    "jungle_book_tokens = word_tokenize(jungle_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(jungle_book_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word_tokenize` returns a `list` object. Lists are useful for storing information, but not for analysing texts. To allow for more refined text analysis we have to convert the list to a NLTK Text object. This type conversion (from list to NLTK text is done below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jungle_nltk_text = nltk.text.Text(jungle_book_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(jungle_nltk_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we tranformed the text into an NLTK Text object we can rely on several handy tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.concordance()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An oft-used technique for distant reading is **Keyword In Context Analysis** in which we centre a whole corpus on a specific word of interest. NLTK comes with a `concordance()` method that allows you to do just this. Let's analyse how the word \"love\" in Jungle book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 9 of 9 matches:\n",
      "in the jungle for fear of those that love thee ; but remember , Akela is very \n",
      "s , and I was born among men , and I love thee , Little Brother . The others t\n",
      "I or Baloo or those of the Pack that love thee . Get the Red Flower . '' By Re\n",
      " bruised from head to foot by me who love him than that he should come to harm\n",
      "e jungles ; and besides , I -- we -- love him , Kaa . '' `` _Ts ! Ts ! _ '' sa\n",
      " and fro . `` I also have known what love is . There are tales I could tell th\n",
      "ever . I will always remember that I love thee and all in our cave ; but also \n",
      "Shere Khan gives me his coat for the love that he bears me . Pull , Gray Broth\n",
      "ce . I am getting old , and I do not love wild elephants , Give me brick eleph\n"
     ]
    }
   ],
   "source": [
    "jungle_nltk_text.concordance(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the number of hits to print with the `lines` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 35 of 35 matches:\n",
      "could handle a buck alone , to young black three-year-olds who thought they cou\n",
      "bs . Who speaks besides Baloo ? '' A black shadow dropped down into the circle \n",
      "o the circle . It was Bagheera , the Black Panther , inky black all over , but \n",
      " Bagheera , the Black Panther , inky black all over , but with the panther mark\n",
      "th his head on Bagheera 's beautiful black skin : `` Little Brother , how often\n",
      "thou_ art a man 's cub , '' said the Black Panther , very tenderly ; '' and eve\n",
      "nly ; and he frowned under his heavy black eyebrows . `` What is the Law of the\n",
      "get up and feed it in the night with black lumps ; and when the morning came an\n",
      "s the boy stood all naked , his long black hair tossing over his shoulders in t\n",
      "than this . Sometimes Bagheera , the Black Panther , would come lounging throug\n",
      " think how small he is , '' said the Black Panther , who would have spoiled Mow\n",
      "? Put dead bats on my head ! Give me black bones to eat ! Roll me into the hive\n",
      "at would the jungle think if I , the Black Panther , curled myself up like Ikki\n",
      "e -- '' '' Is Bagheera , '' said the Black Panther , and his jaws shut with a s\n",
      "a 's light feet on the terrace . The Black Panther had raced up the slope almos\n",
      "e furious din of the fight round the Black Panther -- the yells and chatterings\n",
      "t help chuckling as he heard the big Black Panther asking for help . Kaa had on\n",
      ", remember , Mowgli , I , who am the Black Panther , was forced to call upon Ka\n",
      "ht ; or string a necklace of red and black jungle-nuts ; or watch a lizard bask\n",
      "thou knowest ! '' and the torrent of black horns , foaming muzzles , and starin\n",
      "kites have come down to see it . The black ants have come up to know it . There\n",
      " baby , the night is behind us , And black are the waters that sparkled so gree\n",
      "mashed to pieces against some wicked black cliffs in a heavy sleet-storm with l\n",
      "e beaches , and I am the only seal , black or white , who ever thought of looki\n",
      "ead and spread hood of Nag , the big black cobra , and he was five feet long fr\n",
      "THE ELEPHANTS KALA NAG , which means Black Snake , had served the Indian Govern\n",
      "ighting that Kala Nag , the old wise Black Snake , did not know , for he had st\n",
      "Big Toomai , his driver , the son of Black Toomai who had taken him to Abyssini\n",
      "aught , `` there is nothing that the Black Snake fears except me . He has seen \n",
      "aîl , Kala Nag ! _ ( Go on , go on , Black Snake ! ) _Dant do ! _ ( Give him th\n",
      "pon it , and their shadows were inky black . Little Toomai looked , holding his\n",
      "d came over the moon , and he sat in black darkness ; but the quiet , steady hu\n",
      "ants are beyond the wit of any man , black or white , to fathom . `` Forty year\n",
      "o you suppose I 'm looked after by a black bullock-driver ? '' `` _Huah ! Ouach\n",
      "r of Afghanistan , with his high big black hat of astrakhan wool and the great \n"
     ]
    }
   ],
   "source": [
    "jungle_nltk_text.concordance(\"black\",lines=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "How is the word love used in Romeo and Juliet? The code below downloads Romeo and Juliet, continue the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "randj = requests.get('http://www.gutenberg.org/cache/epub/1112/pg1112.txt').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.similar()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`concordance()` shows words in their context. For example, in Moby Dick the word monstrous occurres in contexts such as the \\_\\_\\_ pictures and a \\_\\_\\_ size. What other words appear in a **similar range of contexts**? We can find out by using the `.similar()` method, entering the word you want analyse within parentheses. Below we aks which words are similar to \"forest\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jungle air world clearing ground pack elephants cave skin sea head\n",
      "other village beasts darkness water monkeys reward beach mule\n"
     ]
    }
   ],
   "source": [
    "jungle_nltk_text.similar(\"forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Research Scenario: How left and right-wing media depict Mueller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research scenario below applies these techniques to understand how the New York times depicts Robert S. Mueller. First we load the data.\n",
    "\n",
    "Don't worry about the technicality on line two: this command basically tells Python to read everything in \"post_message\" columns as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt = pd.read_csv('https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/nytimes.tab',sep='\\t')\n",
    "nyt['post_message'] = nyt['post_message'].astype(str) # this line is a technicality, use it but do not worry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can tokenize the post messages, we have to join them together in one string. `' '.join()` is the opposite of `.split()`. Inspect the examples below to understand how these functions work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you?']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Hello how are you?'\n",
    "words = sentence.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello how are you?'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_merged =  ' '.join(words)\n",
    "sentence_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sentence_merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the NLTK tools to Facebook \"post_messages\" we first select this column, join all the messages into one long string, which is saved in the `posts` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = ' '.join(nyt['post_message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the post messages are stored in one string. This string is tokenized with `word_tokenize` and then convert this list to a NLTK object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts_tokens = word_tokenize(posts)\n",
    "nltk_posts = nltk.text.Text(posts_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 7 of 7 matches:\n",
      " the team of Special Counsel Robert Mueller has been extended Fox News has lear\n",
      "t s time for Special Counsel Robert Mueller to put up or shut up . Amazon could\n",
      "ime to strongly consider firing Bob Mueller . – Dan Bongino https : //bit.ly/2w\n",
      " Duncan said Special Counsel Robert Mueller ’ s team was one holdout juror away\n",
      "ain rally tonight . For months team Mueller has unsuccessfully tried to turn th\n",
      "s brought by Special Counsel Robert Mueller . https : //fxn.ws/2JOF1vN BREAKING\n",
      "tions of former FBI Director Robert Mueller who now leads the special counsel R\n"
     ]
    }
   ],
   "source": [
    "nltk_posts.concordance(\"Mueller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "How does Fox News report about Mueller? Use [these data](https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/foxnews.tab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert these data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\*\\*\\* Exercise\n",
    "\n",
    "Instead of the post messages, compare how commenters on the Facebook page of the New York Times and Fox News write about Mueller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Scenario: Studying changes over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides inspecting the content, we can study changes over time. What are topics salient and when do they disappear. Below we have a look at the comments on the New York Times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at trends, the data has to be sorted in chronological order. We use the timestamp of the comment to sort the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/data_nytimes/page_5281959998_2018_12_28_22_00_39_comments.tab\"\n",
    "nyt_comments = pd.read_csv(url,sep='\\t')\n",
    "nyt_comments_sorted = nyt_comments.sort_values(by='comment_published')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_by</th>\n",
       "      <th>post_text</th>\n",
       "      <th>post_published</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_by</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>comment_message</th>\n",
       "      <th>comment_published</th>\n",
       "      <th>comment_like_count</th>\n",
       "      <th>attachment_type</th>\n",
       "      <th>attachment_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16123</th>\n",
       "      <td>49_408</td>\n",
       "      <td>5281959998_10151785245419999</td>\n",
       "      <td>825d8dd2a27d4f84aec9008bf96872ffb2a4a4ad</td>\n",
       "      <td>Republican leaders gave up hope on Thursday of...</td>\n",
       "      <td>2018-12-27T22:55:00+0000</td>\n",
       "      <td>10151785245419999_10151785253979999</td>\n",
       "      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n",
       "      <td>0</td>\n",
       "      <td>Won’t they just pass a CR to reopen?</td>\n",
       "      <td>2018-12-27T22:56:38+0000</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16417</th>\n",
       "      <td>49_702</td>\n",
       "      <td>5281959998_10151785245419999</td>\n",
       "      <td>825d8dd2a27d4f84aec9008bf96872ffb2a4a4ad</td>\n",
       "      <td>Republican leaders gave up hope on Thursday of...</td>\n",
       "      <td>2018-12-27T22:55:00+0000</td>\n",
       "      <td>10151785245419999_10151785253984999</td>\n",
       "      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n",
       "      <td>0</td>\n",
       "      <td>Damn you, Mexico! You should’ve paid for the s...</td>\n",
       "      <td>2018-12-27T22:56:39+0000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15981</th>\n",
       "      <td>49_266</td>\n",
       "      <td>5281959998_10151785245419999</td>\n",
       "      <td>825d8dd2a27d4f84aec9008bf96872ffb2a4a4ad</td>\n",
       "      <td>Republican leaders gave up hope on Thursday of...</td>\n",
       "      <td>2018-12-27T22:55:00+0000</td>\n",
       "      <td>10151785245419999_10151785254124999</td>\n",
       "      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n",
       "      <td>0</td>\n",
       "      <td>#TrumpShutdown</td>\n",
       "      <td>2018-12-27T22:56:52+0000</td>\n",
       "      <td>115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      position                       post_id  \\\n",
       "16123   49_408  5281959998_10151785245419999   \n",
       "16417   49_702  5281959998_10151785245419999   \n",
       "15981   49_266  5281959998_10151785245419999   \n",
       "\n",
       "                                        post_by  \\\n",
       "16123  825d8dd2a27d4f84aec9008bf96872ffb2a4a4ad   \n",
       "16417  825d8dd2a27d4f84aec9008bf96872ffb2a4a4ad   \n",
       "15981  825d8dd2a27d4f84aec9008bf96872ffb2a4a4ad   \n",
       "\n",
       "                                               post_text  \\\n",
       "16123  Republican leaders gave up hope on Thursday of...   \n",
       "16417  Republican leaders gave up hope on Thursday of...   \n",
       "15981  Republican leaders gave up hope on Thursday of...   \n",
       "\n",
       "                 post_published                           comment_id  \\\n",
       "16123  2018-12-27T22:55:00+0000  10151785245419999_10151785253979999   \n",
       "16417  2018-12-27T22:55:00+0000  10151785245419999_10151785253984999   \n",
       "15981  2018-12-27T22:55:00+0000  10151785245419999_10151785254124999   \n",
       "\n",
       "                                     comment_by  is_reply  \\\n",
       "16123  da39a3ee5e6b4b0d3255bfef95601890afd80709         0   \n",
       "16417  da39a3ee5e6b4b0d3255bfef95601890afd80709         0   \n",
       "15981  da39a3ee5e6b4b0d3255bfef95601890afd80709         0   \n",
       "\n",
       "                                         comment_message  \\\n",
       "16123               Won’t they just pass a CR to reopen?   \n",
       "16417  Damn you, Mexico! You should’ve paid for the s...   \n",
       "15981                                     #TrumpShutdown   \n",
       "\n",
       "              comment_published  comment_like_count attachment_type  \\\n",
       "16123  2018-12-27T22:56:38+0000                   8             NaN   \n",
       "16417  2018-12-27T22:56:39+0000                   1             NaN   \n",
       "15981  2018-12-27T22:56:52+0000                 115             NaN   \n",
       "\n",
       "      attachment_url  \n",
       "16123            NaN  \n",
       "16417            NaN  \n",
       "15981            NaN  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_comments_sorted.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sorting the data, we first make sure all the data in this column is a string. \n",
    "- Then we again join all the comments into one long string. \n",
    "- Tokenize this string.\n",
    "- And convert the list of tokens to a NLTK Text object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyt_comments_sorted['comment_message'] = nyt_comments_sorted['comment_message'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = ' '.join(nyt_comments_sorted['comment_message'])\n",
    "comments_tokens = word_tokenize(comments)\n",
    "nltk_comments = nltk.text.Text(comments_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the comments are stored as a NLTK Text object we can generate a `.dispersion()` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14c80c470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nltk_comments.dispersion_plot(['Merkel','Trump','Mueller'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "How to interpret this figure?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\*\\*\\* Exercise\n",
    "\n",
    "Use posts from the [New York Times](https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/nytimes.tab) and [Fox News](https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/foxnews.tab).\n",
    "For each page:\n",
    "- Sort the data by the posts' Timestamp;\n",
    "- Select the text and `.join()` them into one string\n",
    "- Tokenize the text\n",
    "- Compute word frequencies; compute the relative frequency for the word Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\*\\*\\* Exercise\n",
    "\n",
    "Use posts from the [New York Times](https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/nytimes.tab) and [Fox News](https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/foxnews.tab).\n",
    "For each page:\n",
    "- Sort the data by the post's Timestamp;\n",
    "- Select the text and `.join()` them into one string\n",
    "- Tokenize the text\n",
    "- Compute word frequencies; compute the relative frequency for the word \"Trump\"\n",
    "- Make a dispersion plot for 'Kelly','Cohen','Pelosi', and 'Trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\*\\*\\* Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will investigate the text of Facebook posts in relation to the reactions they received. More specifically we look what makes these audiences angry or happy when these media write about Trump.\n",
    "\n",
    "Use these data\n",
    "- [New York Times]('https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/nytimes.tab')\n",
    "- [Fox News]('https://raw.githubusercontent.com/kasparvonbeelen/CTH2019/master/lecture_3_data/foxnews.tab')\n",
    "\n",
    "So, again for each of these pages:\n",
    "- Compute the ratio of angry and love reactions (by dividing the this reactions type to the total reactions)\n",
    "- Select posts with a ratio of love (or angry) rections higher than 0.3\n",
    "- Save the posts that match these conditions in a new DataFrame (for example `df_nyt_angry`)\n",
    "- Concatenate all these posts into one string, split this string into tokens and convert the list of tokens to a NLTK Text object\n",
    "- Use `.concordance()` to investigate what these supposedly angry or happy posts are about? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now you know everything enough! Congratulations with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Optional Research Scenario: Training an ideological classifier (Advanced Topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part below is optional. It gives code to train an ideology classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score,accuracy_score  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the \"cldata.csv\" on Canvas. This contains rows with texts and labels.\"0\" means that the post was retrieved from the New York Times, \"1\" that comments was writen on the Fox News page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65701</th>\n",
       "      <td>They should remove their license they could ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34276</th>\n",
       "      <td>Gowdy for President. I'm sure he is Republican...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37792</th>\n",
       "      <td>I am 16 months older then my brother.   I can ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13332</th>\n",
       "      <td>Whatever it is, it's creepy and they aren't ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16516</th>\n",
       "      <td>What's with her rigid hand movements when she ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comment_message  label\n",
       "65701  They should remove their license they could ha...      1\n",
       "34276  Gowdy for President. I'm sure he is Republican...      1\n",
       "37792  I am 16 months older then my brother.   I can ...      1\n",
       "13332  Whatever it is, it's creepy and they aren't ge...      1\n",
       "16516  What's with her rigid hand movements when she ...      1"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv('cldata.csv',index_col=0)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To understand how the data was created, consult the appendix below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the data into a train and test set. We use 80% for training, 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103288"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_off = int(all_data.shape[0]*0.8)\n",
    "cut_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,y_train = all_data['comment_message'][:cut_off],all_data['label'][:cut_off]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test,y_test = all_data['comment_message'][cut_off:],all_data['label'][cut_off:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently we use the CountVectorizer to create a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix). To understand the arguments, consult the [CountVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3),max_df=0.9,min_df=20,norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below initializes a classifier model and fits the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = LinearSVC(C=10,class_weight='balanced')\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we calculate labels for the data we separated for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compute how accurarte the classifier performed on these test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7377817365037564\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(predictions,y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### \\*\\*\\* Building an Emotion Classifier (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "\n",
    "Code to create classifier data from Netvizz data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def count_tokens(string):\n",
    "    tokens = wordpunct_tokenize(str(string))\n",
    "    return len(tokens)\n",
    "\n",
    "nyt = pd.read_csv('../classifierdata/nyt_comments.tab',sep='\\t')\n",
    "fn = pd.read_csv('../classifierdata/fn_comments.tab',sep='\\t')\n",
    "nyt['comment_message_length'] = nyt['comment_message'].apply(count_tokens)\n",
    "fn['comment_message_length'] = fn['comment_message'].apply(count_tokens)\n",
    "nyt_long = nyt[nyt['comment_message_length'] > 10]\n",
    "fn_long = fn[fn['comment_message_length'] > 10]\n",
    "fn_long_reduced = fn_long.iloc[:nyt_long.shape[0],:]\n",
    "all_data = pd.concat([nyt_long,fn_long_reduced])\n",
    "labels = np.asarray([0]*nyt_long.shape[0] + [1]*fn_long_reduced.shape[0])\n",
    "all_data.shape[0]==labels.shape[0]\n",
    "all_data['label'] = labels\n",
    "order = np.asarray(range(0,all_data.shape[0]))\n",
    "random.shuffle(order)\n",
    "all_data = all_data.iloc[order,:]\n",
    "cldata = all_data.loc[:,['comment_message','label']]\n",
    "cldata.to_csv('cldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
